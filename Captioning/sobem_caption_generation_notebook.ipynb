{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Antarctic-Captions",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v22KSBpk6FDP"
      },
      "source": [
        "## Image Caption Generation Demo\n",
        "\n",
        "By: dzryk (discord, https://twitter.com/dzryk, https://github.com/dzryk)\n",
        "\n",
        "This notebook provides an image captioning demo that goes along with the antarctic-captions repository (https://github.com/dzryk/antarctic-captions)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLyeSeDd6ULN"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ld1VDA2C6Xul"
      },
      "source": [
        "!git clone https://github.com/dzryk/antarctic-captions.git\n",
        "%cd antarctic-captions/\n",
        "!git clone https://github.com/openai/CLIP"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdhHZXxE6qqT"
      },
      "source": [
        "!pip3 install gdown\n",
        "!pip3 install ftfy\n",
        "!pip3 install transformers\n",
        "!pip3 install git+https://github.com/PyTorchLightning/pytorch-lightning"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bePz8ETA634r"
      },
      "source": [
        "# Download models and cache\n",
        "!wget -m -np -c -U \"eye02\" -w 2 -R \"index.html*\" \"https://the-eye.eu/public/AI/models/antarctic-captions/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vUHiYJT7NnJ"
      },
      "source": [
        "import argparse\n",
        "import io\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import requests\n",
        "import pytorch_lightning as pl\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.transforms.functional as F\n",
        "\n",
        "from CLIP import clip\n",
        "from PIL import Image\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "import model\n",
        "import utils"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KcWkLT5x7pad"
      },
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kIg55IT8qmy"
      },
      "source": [
        "# Helper functions\n",
        "def fetch(url_or_path):\n",
        "    if str(url_or_path).startswith('http://') or str(url_or_path).startswith('https://'):\n",
        "        r = requests.get(url_or_path)\n",
        "        r.raise_for_status()\n",
        "        fd = io.BytesIO()\n",
        "        fd.write(r.content)\n",
        "        fd.seek(0)\n",
        "        return fd\n",
        "    return open(url_or_path, 'rb')\n",
        "\n",
        "def load_image(img, preprocess):\n",
        "    img = Image.open(fetch(img))\n",
        "    return img, preprocess(img).unsqueeze(0).to(device)\n",
        "\n",
        "def show(imgs):\n",
        "    if not isinstance(imgs, list):\n",
        "        imgs = [imgs]\n",
        "    fix, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n",
        "    for i, img in enumerate(imgs):\n",
        "        img = img.detach()\n",
        "        img = F.to_pil_image(img)\n",
        "        axs[0, i].imshow(np.asarray(img))\n",
        "        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
        "\n",
        "def display_grid(imgs):\n",
        "    reshaped = [F.to_tensor(x.resize((256, 256))) for x in imgs]\n",
        "    show(make_grid(reshaped))\n",
        "    \n",
        "def clip_rescoring(args, net, candidates, x):\n",
        "    textemb = net.perceiver.encode_text(\n",
        "        clip.tokenize(candidates).to(args.device)).float()\n",
        "    textemb /= textemb.norm(dim=-1, keepdim=True)\n",
        "    similarity = (100.0 * x @ textemb.T).softmax(dim=-1)\n",
        "    _, indices = similarity[0].topk(args.num_return_sequences)\n",
        "    return [candidates[idx] for idx in indices[0]]\n",
        "\n",
        "def loader(args):\n",
        "    cache = []\n",
        "    with open(args.textfile) as f:\n",
        "        for line in f:\n",
        "            cache.append(line.strip())\n",
        "    cache_emb = np.load(args.embfile)\n",
        "    net = utils.load_ckpt(args)\n",
        "    net.cache = cache\n",
        "    net.cache_emb = torch.tensor(cache_emb).to(args.device)\n",
        "    preprocess = clip.load(args.clip_model, jit=False)[1]\n",
        "    return net, preprocess\n",
        "    \n",
        "def caption_image(path, args, net, preprocess):\n",
        "    captions = []\n",
        "    img, mat = load_image(path, preprocess)\n",
        "    table, x = utils.build_table(mat.to(device), \n",
        "                          perceiver=net.perceiver,\n",
        "                          cache=net.cache,\n",
        "                          cache_emb=net.cache_emb,\n",
        "                          topk=args.topk,\n",
        "                          return_images=True)\n",
        "    table = net.tokenizer.encode(table[0], return_tensors='pt').to(device)\n",
        "    out = net.model.generate(table,\n",
        "                             do_sample=args.do_sample,\n",
        "                             num_beams=args.num_beams,\n",
        "                             temperature=args.temperature,\n",
        "                             top_p=args.top_p,\n",
        "                             num_return_sequences=args.num_return_sequences)\n",
        "    candidates = []\n",
        "    for seq in out:\n",
        "        candidates.append(net.tokenizer.decode(seq, skip_special_tokens=True))\n",
        "    captions = clip_rescoring(args, net, candidates, x[None,:])\n",
        "    #for c in captions[:args.display]:\n",
        "        #print(c)\n",
        "    display_grid([img])\n",
        "    return captions"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMtvC3LE8vkw"
      },
      "source": [
        "# Settings\n",
        "filedir='the-eye.eu/public/AI/models/antarctic-captions/'\n",
        "args = argparse.Namespace(\n",
        "    ckpt=f'{filedir}/-epoch=05-vloss=2.163.ckpt',\n",
        "    textfile=f'{filedir}/postcache.txt',\n",
        "    embfile=f'{filedir}/postcache.npy',\n",
        "    clip_model='ViT-B/32',\n",
        "    topk=10,\n",
        "    num_return_sequences=1000,\n",
        "    num_beams=1,\n",
        "    temperature=1.0,\n",
        "    top_p=1.0,\n",
        "    display=1000,\n",
        "    do_sample=True,\n",
        "    device=device\n",
        ")"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1zzlu3c9FQ1"
      },
      "source": [
        "# Load checkpoint and preprocessor\n",
        "net, preprocess = loader(args)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "6O64hUPYUhus"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from os import listdir\n",
        "\n",
        "#Path to which to write the captions\n",
        "WRITE_PATH = f'/content/drive/My Drive/sobem/Captions_Test/'\n",
        "\n",
        "#Iterate over the 10 image subjects\n",
        "for i in range(1,11):\n",
        "\n",
        "  #Get path of all images for each subject\n",
        "  target_path = f'/content/drive/My Drive/sobem/Photos/{i}/'\n",
        "  imgs = listdir(target_path)\n",
        "\n",
        "  #Caption each image of the subject\n",
        "  for image in imgs:\n",
        "    img = f'{target_path}{image}'\n",
        "    captions = caption_image(img, args, net, preprocess)\n",
        "\n",
        "    #Write the captions to file\n",
        "    write_string = '\\n'.join(captions)\n",
        "    with open(f'{WRITE_PATH}{image[:-4]}.txt','w') as writer:\n",
        "      writer.write(write_string)\n",
        "    \n",
        "    print(write_string)"
      ],
      "metadata": {
        "id": "6JbGNK0CVIHO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}