{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nik10e0gGEGm",
        "outputId": "64b79ce8-b5a1-4d7d-f5b4-547c841268f7"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi\n",
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X--QLtz1NXn6",
        "outputId": "1a7d2e24-788b-4a2f-a493-70ecce9aadc4"
      },
      "outputs": [],
      "source": [
        "!pip install open_clip_torch matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "xf8jcU3WNby5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xyOOIJTqNfCD",
        "outputId": "cbda8a00-e75f-4d70-b4c6-6b0d10a1cd45"
      },
      "outputs": [],
      "source": [
        "import open_clip\n",
        "open_clip.list_pretrained()\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32-quickgelu', pretrained='laion400m_e32')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uGzW43YqNkem",
        "outputId": "c799c973-461d-4203-9a9c-a4d67ba1eee5"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "context_length = model.context_length\n",
        "vocab_size = model.vocab_size\n",
        "\n",
        "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
        "print(\"Context length:\", context_length)\n",
        "print(\"Vocab size:\", vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vAU-N6D2T3J3",
        "outputId": "908a7512-453f-4747-8c17-d739ae7ba9ea"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Sy7ntrRvNo9n"
      },
      "outputs": [],
      "source": [
        "from open_clip import tokenize\n",
        "import pandas as pd\n",
        "\n",
        "#Define text stimuli\n",
        "female = ['female', 'woman', 'girl', 'sister', 'she', 'her', 'hers', 'daughter']\n",
        "male = ['male', 'man', 'boy', 'brother', 'he', 'him', 'his', 'son']\n",
        "all_words = female + male\n",
        "\n",
        "embeddings = []\n",
        "\n",
        "for word in all_words:\n",
        "  with torch.no_grad():\n",
        "    tokens = tokenize([word])\n",
        "    emb = model.encode_text(tokens).numpy().squeeze()\n",
        "    embeddings.append(emb)\n",
        "\n",
        "emb_arr = np.array(embeddings)\n",
        "emb_df = pd.DataFrame(emb_arr,index=all_words)\n",
        "emb_df.to_csv(f'/content/drive/My Drive/open_clip/clip_openclip_emb_df_language.vec',sep=' ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "chlmfthbNzXy",
        "outputId": "6f93eb5e-a4d8-4dfe-b0a0-858ec70ee227"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "from os import listdir\n",
        "\n",
        "SOURCE_DIR = f'/content/drive/My Drive/OASIS/images/'\n",
        "\n",
        "targets = listdir(SOURCE_DIR)\n",
        "embeddings = []\n",
        "\n",
        "for target in targets:\n",
        "\n",
        "  print(target)\n",
        "\n",
        "  img = Image.open(f'{SOURCE_DIR}{target}').convert('RGB')\n",
        "  with torch.no_grad():\n",
        "    input_ = preprocess(img).unsqueeze(0)\n",
        "    emb = model.encode_image(input_).numpy().squeeze()\n",
        "    embeddings.append(emb)\n",
        "\n",
        "emb_arr = np.array(embeddings)\n",
        "emb_df = pd.DataFrame(emb_arr,index=targets)\n",
        "emb_df.to_csv(f'/content/drive/My Drive/open_clip/clip_openclip_emb_df.vec',sep=' ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G0rSaS1mQpRE"
      },
      "outputs": [],
      "source": [
        "#Define text stimuli\n",
        "person = ['person','woman','human','human being','individual','adult']\n",
        "happy = ['happy person','happy woman','happy human','happy human being','happy individual','happy adult']\n",
        "sad = ['sad person','sad woman','sad human','sad human being','sad individual','sad adult']\n",
        "angry = ['angry person','angry woman','angry human','angry human being','angry individual','angry adult']\n",
        "\n",
        "person_ = person + [f'a {i}' for i in person] + [f'a photo of a {i}' for i in person] + [f'an image of a {i}' for i in person] + [f'a picture of a {i}' for i in person]\n",
        "happy_ = happy + [f'a {i}' for i in happy] + [f'a photo of a {i}' for i in happy] + [f'an image of a {i}' for i in happy] + [f'a picture of a {i}' for i in happy]\n",
        "sad_ = sad + [f'a {i}' for i in sad] + [f'a photo of a {i}' for i in sad] + [f'an image of a {i}' for i in sad] + [f'a picture of a {i}' for i in sad]\n",
        "angry_ = angry + [f'an {i}' for i in angry] + [f'a photo of an {i}' for i in angry] + [f'an image of an {i}' for i in angry] + [f'a picture of an {i}' for i in angry]\n",
        "\n",
        "all_words = person_ + happy_ + sad_ + angry_\n",
        "\n",
        "embeddings = []\n",
        "\n",
        "for word in all_words:\n",
        "  with torch.no_grad():\n",
        "    tokens = tokenize([word])\n",
        "    emb = model.encode_text(tokens).numpy().squeeze()\n",
        "    embeddings.append(emb)\n",
        "\n",
        "emb_arr = np.array(embeddings)\n",
        "emb_df = pd.DataFrame(emb_arr,index=all_words)\n",
        "emb_df.to_csv(f'/content/drive/My Drive/open_clip/sobem_clip_openclip_emb_lang_df.vec',sep=' ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BhhMpNg4N8R4"
      },
      "outputs": [],
      "source": [
        "SOURCE_DIR = f'/content/drive/My Drive/sobem/Photos/'\n",
        "\n",
        "embeddings, df_index = [],[]\n",
        "\n",
        "for i in range(1,11):\n",
        "  target_dir = f'{SOURCE_DIR}{str(i)}/'\n",
        "  targets = listdir(target_dir)\n",
        "\n",
        "  for target in targets:\n",
        "\n",
        "    print(target)\n",
        "\n",
        "    img = Image.open(f'{target_dir}{target}')\n",
        "    with torch.no_grad():\n",
        "      input_ = preprocess(img).unsqueeze(0)\n",
        "      emb = model.encode_image(input_).numpy().squeeze()\n",
        "      embeddings.append(emb)\n",
        "      df_index.append(target)\n",
        "\n",
        "emb_arr = np.array(embeddings)\n",
        "emb_df = pd.DataFrame(emb_arr,index=df_index)\n",
        "emb_df.to_csv(f'/content/drive/My Drive/open_clip/sobem_clip_openclip_emb_df.vec',sep=' ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dtWzxD2yRBqq"
      },
      "outputs": [],
      "source": [
        "#Define text stimuli\n",
        "sex = ['person to have intercourse with','person to be intimate with','person to have sex with','person to kiss','person to undress','person to have coitus with']\n",
        "sex_person = sex + [f'a {i}' for i in sex] + [f'a photo of a {i}' for i in sex] + [f'an image of a {i}' for i in sex] + [f'a picture of a {i}' for i in sex]\n",
        "\n",
        "science = ['scientist','researcher','engineer','physicist','mathematician','chemist']\n",
        "sci_person = science + [f'a {i}' for i in science] + [f'a photo of a {i}' for i in science] + [f'an image of a {i}' for i in science] + [f'a picture of a {i}' for i in science]\n",
        "\n",
        "business = ['businessperson', 'business leader', 'manager', 'executive', 'CEO', 'chief executive officer']\n",
        "bus_person = business + [f'a {i}' for i in business] + [f'a photo of a {i}' for i in business] + [f'an image of a {i}' for i in business] + [f'a picture of a {i}' for i in business]\n",
        "\n",
        "medicine = ['doctor', 'physician', 'clinician','surgeon', 'medical expert', 'health professional']\n",
        "med_person = medicine + [f'a {i}' for i in medicine] + [f'a photo of a {i}' for i in medicine] + [f'an image of a {i}' for i in medicine] + [f'a picture of a {i}' for i in medicine]\n",
        "\n",
        "all_words = sex_person + sci_person + bus_person + med_person\n",
        "\n",
        "embeddings = []\n",
        "\n",
        "for word in all_words:\n",
        "  with torch.no_grad():\n",
        "    tokens = tokenize([word])\n",
        "    emb = model.encode_text(tokens).numpy().squeeze()\n",
        "    embeddings.append(emb)\n",
        "\n",
        "emb_arr = np.array(embeddings)\n",
        "emb_df = pd.DataFrame(emb_arr,index=all_words)\n",
        "emb_df.to_csv(f'/content/drive/My Drive/open_clip/profession_clip_openclip_emb_lang_df.vec',sep=' ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BtRy2NFlRo8X"
      },
      "outputs": [],
      "source": [
        "#Get CLIP image embeddings\n",
        "SOURCE_DIR = f'/content/drive/My Drive/professional_stimuli/'\n",
        "df_index = []\n",
        "image_embeddings = []\n",
        "\n",
        "professions = ['ceo','doctor','scientist']\n",
        "genders = ['men','women']\n",
        "\n",
        "for profession in professions:\n",
        "    for gender in genders:\n",
        "        target_dir = f'{SOURCE_DIR}{profession}/{gender}'\n",
        "        imgs = listdir(target_dir)\n",
        "\n",
        "        for idx,img in enumerate(imgs):\n",
        "            image = preprocess(Image.open(f'{target_dir}/{img}').convert('RGB')).unsqueeze(0).to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                emb = model.encode_image(image).cpu().numpy().squeeze()\n",
        "                image_embeddings.append(emb)\n",
        "                df_index.append(f'{profession}_{gender}_{img}')\n",
        "\n",
        "image_array = np.array(image_embeddings)\n",
        "image_df = pd.DataFrame(image_array,index=df_index)\n",
        "image_df.to_csv(f'/content/drive/My Drive/open_clip/profession_clip_openclip_emb_df.vec',sep=' ')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "openclip_embedding_collector.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
